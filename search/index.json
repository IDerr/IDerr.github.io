[{"content":"Introduction Want to connect your AWX/Ansible Tower with your SSO solution (thanks to openid connect), in my case keycloak, you\u0026rsquo;re in the right place. I have seen a lot of tutorials on how to configure with SAML, but not one with OIDC so here it is :)\nPrerequisites A keycloak An ansible tower / awx If you respect all these prerequisites, you are good to go !\nTutorial Go to your beautiful keycloak instance Add a client in keycloak with this redirect url https://\u0026lt;AWX_HOST\u0026gt;/sso/complete/oidc/ Something like this :\nconfig Get your keys in your credentials part (if not set, set it to Client ID and secret)\nIn your awx/tower instance, go to settings, generic oidc and fill all the infos : OIDC key : Keycloak\u0026rsquo;s client id OIDC secret : Keycloak\u0026rsquo;s client secret OIDC provider : https://\u0026lt;KEYCLOAK_HOST\u0026gt;/realms/\u0026lt;YOUR_REALM\u0026gt;\nConclusion Only that, yes. login Your awx/ansible tower is now connected, login and do as you please!\nSee you on the next article !\n","date":"2022-11-04T00:00:00Z","image":"https://iderr.github.io/background.jpg","permalink":"https://iderr.github.io/p/connect-your-awx-ansible-tower-with-keycloak/","title":"Connect your AWX - Ansible Tower with Keycloak !"},{"content":"Introduction Want to build your own EKS, GKE on vanilla Kubernetes ? Here\u0026rsquo;s an easy way to use Kubermatic as your kubernetes provider on your infra.\nKubermatic will manage all your kubernetes clusters using your favorite hypervisor, with a great multi-tenant UI.\nPrerequisites To make your own Kubernetes as a Service you need :\nAt least one kubernetes cluster, will be used to host control plane services and kubermatic software\nAt least one infrastructure provider (https://github.com/kubermatic/machine-controller)\nAWS GCP Openstack Hetzner Kubevirt Digitalocean Azure Nutanix VMWare Cloud Director VMWare Vsphere Linode Using one of the following operating system (Full compatibility here : https://github.com/kubermatic/machine-controller/blob/master/docs/operating-system.md): Ubuntu Flatcar CentOS 7 If you respect all these prerequisites, you are good to go\nArchitecture Kubermatic has a 3 layer architecture : Master\nMaster Cluster Seed Cluster User Cluster We will go through each of them to detail their usage\nMaster Cluster Master cluster is reponsible for running all kubermatic related software :\nWeb UI aka Dashboard Kubermatic API Machine Controller responsible to deploy and manage worker nodes Seed Cluster Seed cluster is reponsible for running all control planes software for user clusters Each user cluster will use a different namespace in the seed cluster to separate resources from each cluster.\nUser Cluster The user cluster is the cluster created for a tenant.\nArchitecture Simple architecture You\u0026rsquo;ll need one kubernetes cluster for this simple architecture, master and seed cluster will be grouped in one cluster. The user cluster will be instanciated in your provider.\nsimple Large scale architecture You\u0026rsquo;ll need at least two kubernetes clusters for this. Master will be hosted in a cluster and the seed cluster in another. You can separate your clusters in multiple seeds depending on the granularity needed (Country/Datacenter or other)\nThe user cluster will still be instanciated in your provider.\nConclusion Now that you know the architecture of Kubermatic, we\u0026rsquo;ll install it on a kubernetes cluster linked to an openstack cluster !\nSee you on the next article !\n","date":"2022-08-03T00:00:00Z","image":"https://iderr.github.io/p/deploy-your-kubernetes-as-a-service-using-kubermatic-part-1/background_hu563d7888e6619e9fd1d896a7de8ceb45_118378_120x120_fill_q75_box_smart1.jpg","permalink":"https://iderr.github.io/p/deploy-your-kubernetes-as-a-service-using-kubermatic-part-1/","title":"Deploy your kubernetes as a service using Kubermatic ! Part 1"},{"content":"Introduction In the last article, we have imported OpenFoodFacts data from a CSV format to PostgreSQL.\nNow, our data is ready to be analysed by anyone. SQL can be great for analysis but if we want to build an application, we\u0026rsquo;ll need another transport layer like REST or GraphQL. So in this article, I\u0026rsquo;ll show you how create a quick GraphQL API using Hasura\nYou can find all my work on my Github : https://github.com/IDerr/Openfoodfacts\nWhat is Hasura Hasura is a GraphQL engine that can be connected to your database and result in a unified data access layer, here GraphQL.\nIf you\u0026rsquo;re interested by the project, you can find a lot more informations on their website !\nInstallation Installation of hasura is pretty straight forward, I\u0026rsquo;ll use Docker-compose to simplify this process.\nHere\u0026rsquo;s the docker-compose I\u0026rsquo;ve used to create my hasura engine\nversion: \u0026#39;3.6\u0026#39;\rservices:\rgraphql-engine:\rimage: hasura/graphql-engine:v2.0.10\rports:\r- 8080:8080\rrestart: always\renvironment:\r## postgres database to store Hasura metadata\rHASURA_GRAPHQL_METADATA_DATABASE_URL: postgresql://username:password@hostname:5432/mymetadatadatabase\r## this env var can be used to add the above postgres database to Hasura as a data source. this can be removed/updated based on your needs\rPG_DATABASE_URL: postgresql://username:password@hostname:5432/openfoodfacts\r## enable the console served by server\rHASURA_GRAPHQL_ENABLE_CONSOLE: \u0026#34;true\u0026#34; # set to \u0026#34;false\u0026#34; to disable console\r## enable debugging mode. It is recommended to disable this in production\rHASURA_GRAPHQL_DEV_MODE: \u0026#34;true\u0026#34;\rHASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log\r## uncomment next line to set an admin secret\r# HASURA_GRAPHQL_ADMIN_SECRET: myadminsecretkey Now you run the tool easily with\ndocker-compose up -d 1.jpg TADAA, you know have a hasura engine ready, just type http://youhostname:8080/console You\u0026rsquo;ll be welcomed with a super UI.\nYou can now go to the \u0026ldquo;Data\u0026rdquo; section where you can setup the relationship between your database and Hasura.\n2.jpg Just track everything to link all tables !\nThen it\u0026rsquo;ll automatically detect all the links between tables, track them also.\n3.jpg After that, you can come back in the \u0026ldquo;API\u0026rdquo; section and your API is ready !\nWe can now easily create our GraphQL requests, for example all products that doesn\u0026rsquo;t have a creator\nquery GetProductsWithoutCreator { Product(where: {creator: {_is_null: true}}){ code creator product_name } } 4.jpg Oh, we have found 4 products without any creator, weird\u0026hellip;\nWe can now report that to the OpenFood facts team :)\nAnalysis can be done very quicky with tools like this, and if we want to build an Android app, our API will be ready to go !\nNext steps Next steps, develop more GraphQL queries to find inconsistent data \u0026amp; improve the data model. I\u0026rsquo;ll focus on that now !\nAlso, that we have our API, we can try to make a simple app to try the integration with Hasura.\nConclusion I\u0026rsquo;ll continue working on my side, if anyone would like to help me on this project, feel free to create issues and PR on the github repository. I am also available on Openfoodfacts Slack (IDerr), if there\u0026rsquo;s any questions!\nSee you next week for the next article (on OpenFoodFacts)!\n","date":"2021-11-19T00:00:00Z","image":"https://iderr.github.io/p/week-2-using-hasura-to-analyze-openfoodfacts/background_hu7f4aefc3a426d3b4990b4956a4ec486a_256879_120x120_fill_box_smart1_3.png","permalink":"https://iderr.github.io/p/week-2-using-hasura-to-analyze-openfoodfacts/","title":"Week 2 - Using Hasura to analyze OpenFoodFacts"},{"content":"Introduction Today we will discuss how to import OpenFoodFacts data from their CSV to a PostgreSQL Database.\nI wanted to work on OpenFoodFacts data, but never found MongoDB really great to work with, so I decided to migrate the CSV Data to Postgres, and clean the database (and give back a bit to this awesome community)\nYou can find all my work on my Github : https://github.com/IDerr/Openfoodfacts\nHow to download OpenFoodFacts data OpenFoodFacts provides all their data for free on their website, with three different formats: Json (based on their mongodb database), CSV and RDF You can download al the data on their website: https://openfoodfacts.org/data\nFor the sake of simplicity, I\u0026rsquo;ll use the CSV format for my project.\nHow to import data from the CSV to PostgreSQL As I said, I\u0026rsquo;ll store my data on PostgreSQL, I think this is a great way to store linked data like this (A graph database would be a good fit also), it\u0026rsquo;ll enforce that all fields are filled and the schema respected.\nI took the CSV file, checked all the columns available, and went with a simple schema (still can be improved :D).\nI have determined 12 Models (Additives, Allergens, Brand, Category, City, Country, Emb, Label, Package, Product, Store, Trace) and a bunch of Pivot tables for many to many relations.\nTo implement models files, I used SQLAlchemy to create my database schema programmatically, so I can reuse those models files if I want to interact with the database.\nA small python script to parse the CSV, remove incorrect data (same barcode on multiple products ?), and create the product with full links, and tada, we have our database with a lot of products!\ncount Next steps Now that our database is ready, there is still work that can be done. The first step I see is cleaning possible errors or inconsistent data. We have multiple ways to analyse the database to find these errors:\nUse the CSV with some Python to detect anomalies (Pandas + some code) Use SQL to find anomalies (or again use Pandas with SQL as a backend) After that part, we can make an API based on the models we created before, to integrate the database with our applications!\nFinally, finding ways to improve our database, translations, ingredients, score and so on!\nConclusion I\u0026rsquo;ll continue working this project on my side, if anyone would like to help me on this project, feel free to create issues and PR on the github repository. I am also available on Openfoodfacts Slack (IDerr), if there\u0026rsquo;s any questions!\nIf anyone from openfoodfacts is interested on merging the work I am doing on the public database, don\u0026rsquo;t hesitate! See you next week for the next article (on OpenFoodFacts)!\n","date":"2021-11-14T00:00:00Z","image":"https://iderr.github.io/p/week-1-new-project-on-openfoodfacts/background_hu49c03114a9ce8ad2bc0bd62c0ddb3f2a_750790_120x120_fill_box_smart1_3.png","permalink":"https://iderr.github.io/p/week-1-new-project-on-openfoodfacts/","title":"Week 1 - New project on OpenFoodFacts"},{"content":"Introduction Today, we will discuss how to use Tutor for deploying OpenEdx. OpenEdx is a platform used to deploy your courses online, it\u0026rsquo;s an Udemy like but self hosted.\nTutor is an amazing tool to get it deployed with Docker (or Kubernetes).\nInstallation Tutor installation Installation is really straightforward, a simple binary to download and you\u0026rsquo;re done !\nsudo curl -L \u0026#34;https://github.com/overhangio/tutor/releases/download/v12.1.6/tutor-$(uname -s)_$(uname -m)\u0026#34; -o /usr/local/bin/tutor sudo chmod 0755 /usr/local/bin/tutor Shamefully copied/pasted from the documentation\nReally simple, no ?\nRun your environnement Now that we have Tutor installed in our environnement, we can launch our platform with the following command\ntutor local quickstart You\u0026rsquo;ll be asked some questions to change some variables :\nIs it a production platform ? Our instance name Our contact email For this tutorial, I\u0026rsquo;ll run a non production openedx.\niderr@iderr-VirtualBox:~$ tutor local quickstart ================================================== Interactive platform configuration ================================================== Are you configuring a production platform? Type \u0026#39;n\u0026#39; if you are just testing Tutor on your local computer [Y/n] n As you are not running this platform in production, we automatically set the following configuration values: LMS_HOST = local.overhang.io CMS_HOST = studio.local.overhang.io ENABLE_HTTPS = False Your platform name/title [My Open edX] platform1 Your public contact email address [contact@local.overhang.io] The default language code for the platform [en] Configuration saved to /home/iderr/.local/share/tutor/config.yml Environment generated in /home/iderr/.local/share/tutor/env We let tutor do its job, and your environnement will be ready after seeing a text message like this\nAll services initialised.\rThe Open edX platform is now running in detached mode\rYour Open edX platform is ready and can be accessed at the following urls:\rhttp://local.overhang.io\rhttp://studio.local.overhang.io For our test, I\u0026rsquo;ll stick with defaults urls, of course, you can use yours. I\u0026rsquo;ll modify my /etc/hosts file to redirect urls in my localhost environnement\n127.0.0.1\tlocalhost local.overhang.io studio.local.overhang.io TADA Success Conclusion Congratulations, now, we have a running OpenEdx installation ! See you next time for the configuration of OpenEdx and its customisation !\n","date":"2021-11-07T00:00:00Z","image":"https://iderr.github.io/p/introduction-to-tutor/background_hu4902f2b049033e1a8195559260a8ee86_735658_120x120_fill_box_smart1_3.png","permalink":"https://iderr.github.io/p/introduction-to-tutor/","title":"Introduction to Tutor"},{"content":"Introduction First article of this new blog!\nToday we will discuss how to configure automatic Letâ€™s-Encrypt certificate renewal with a domain hosted in OVH.\nI have not found a clear tutorial on how to setup a cluster wide OVH cert-manager provider so there it is.\nInstallation Cert-manager installation Quick reminder, installing cert-manager is pretty straightforward with Helm. Don\u0026rsquo;t forget to replace the version with the latest one : https://github.com/jetstack/cert-manager/releases\nkubectl create namespace cert-manager helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.5.3 --set installCRDs=true After that, you should have a running cert-manager.\nOVH Webhook installation git clone https://github.com/baarde/cert-manager-webhook-ovh.git cd cert-manager-webhook-ovh helm install cert-manager-webhook-ovh ./deploy/cert-manager-webhook-ovh --set groupName=\u0026#39;\u0026lt;GROUP_NAME\u0026gt;\u0026#39; After that, we need to create our api keys in the OVH API to connect our webhook controller to OVH\nGo to https://api.ovh.com/createToken/index.cgi Add the followings rights, if you want to give acces to all of your domains GET /domain/zone/* PUT /domain/zone/* POST /domain/zone/* DELETE /domain/zone/* If you prefer to give access only to one domain replace the \u0026ldquo;*\u0026rdquo; by your domain name We will store the freshly generated application secret in Kubernetes.\nThe secret needs to be in the same namespace as the cert-manager controller pod if you want to create a ClusterIssuer, in our case, \u0026lsquo;cert-manager\u0026rsquo;\nkubectl create secret generic ovh-credentials --namespace cert-manager --from-literal=applicationSecret=\u0026#39;\u0026lt;OVHSECRET\u0026gt;\u0026#39; Grant permission to get the secret to the cert-manager-webhook-ovh service account\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cert-manager-webhook-ovh:secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] resourceNames: [\u0026#34;ovh-credentials\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cert-manager-webhook-ovh:secret-reader roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cert-manager-webhook-ovh:secret-reader subjects: - apiGroup: \u0026#34;\u0026#34; namespace: default kind: ServiceAccount name: cert-manager-webhook-ovh And we can finally create our cluster issuer, don\u0026rsquo;t forget to replace the values between \u0026lt;\u0026gt; with your keys/config\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt namespace: cert-manager spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: \u0026#39;\u0026lt;EMAIL\u0026gt;\u0026#39; privateKeySecretRef: name: letsencrypt-account-key solvers: - dns01: webhook: groupName: \u0026#39;\u0026lt;GROUP_NAME\u0026gt;\u0026#39; solverName: ovh config: endpoint: ovh-eu applicationKey: \u0026#39;\u0026lt;APP_KEY\u0026gt;\u0026#39; applicationSecretRef: key: applicationSecret name: ovh-credentials consumerKey: \u0026#39;\u0026lt;CONSUMER_KEY\u0026gt;\u0026#39; And voila, you have a fully working ClusterIssuer with OVH, you can test all your work with a new Certificate.\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: example-certificate spec: dnsNames: - test.mydomain.com issuerRef: name: letsencrypt kind: ClusterIssuer secretName: test-mydomain-com-tls NAME READY SECRET AGE example-certificate True test-mydomain-com-tls 3s Conclusion Congratulations, and see you next time for another article!\n","date":"2021-08-27T00:00:00Z","image":"https://cdna.artstation.com/p/assets/images/images/009/211/358/large/bala-vidhya-sagar-1.jpg?1517728575","permalink":"https://iderr.github.io/p/ovh-provider-for-cert-manager-dns-01/","title":"OVH provider for cert-manager DNS-01"}]